<!DOCTYPE html>
<html lang="en-us">
  <head><link rel="icon" href="../../images/chicken.png"><meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
<meta http-equiv="x-ua-compatible" content="ie=edge" /><meta property="og:url"  content="/posts/constrained_wireless/" />
    <meta property="og:type" content="article" /><meta property="og:title" content="Constrained Learning for Wireless Systems" /><meta property="og:description" content="" /><meta property="og:image:width"  content="600" />
        <meta property="og:image:height" content="455" /><meta property="og:image" content="/posts/constrained_wireless/images/cap_lambd_hu2887ca7646cd42e51b7ffd8624bbbc77_28779_600x0_resize_box_2.png" /><title>Clark Zhang - Constrained Learning for Wireless Systems</title>

    
<link href="https://fonts.googleapis.com/css?family=Open&#43;Sans" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=Open&#43;Sans" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=Open&#43;Sans" rel="stylesheet"><link rel="stylesheet" type="text/css" href="../../css/style.min.f262a78d45f49ef09d15d142086c13590445f037a703ae9b6bf3b773e5075bb9.css" integrity="sha256-8mKnjUX0nvCdFdFCCGwTWQRF8DenA66ba/O3c&#43;UHW7k=">
<link rel="stylesheet" type="text/css" href="../../css/monokai-sublime.9.15.8.min.min.3a5d282f03108101d715e80fd4c07b55502ec4673fc063f2b6e415d4def5b354.css" integrity="sha256-Ol0oLwMQgQHXFegP1MB7VVAuxGc/wGPytuQV1N71s1Q=">
<link rel="stylesheet" type="text/css" href="../../css/icons.min.ee23494515027c9d455d659dd0760f472e9c02e16079f933928bc4820d5b9e58.css" integrity="sha256-7iNJRRUCfJ1FXWWd0HYPRy6cAuFgefkzkovEgg1bnlg=">
<link rel="stylesheet" type="text/css" href="../../css/refresh.min.dc1f32b382057eb84921b684213bdba80b0d433fbe9de934059cf321c8fb4bac.css" integrity="sha256-3B8ys4IFfrhJIbaEITvbqAsNQz&#43;&#43;nek0BZzzIcj7S6w=">
<link rel="stylesheet" type="text/css" href="../../css/devicon.min.min.13fddf418611477a10293a87542de7703396184696bd8cf50642c4c72994c78a.css" integrity="sha256-E/3fQYYRR3oQKTqHVC3ncDOWGEaWvYz1BkLExymUx4o=">
    

  </head>
  <body>
     

    <div id="preloader">
      <div id="status"></div>
    </div><nav class="navbar is-fresh is-transparent no-shadow" role="navigation" aria-label="main navigation">
  <div class="container">
    <div class="navbar-brand">

      
      
      
      <a class="navbar-item">
        <div class="menu-icon-wrapper left-menu-icon-wrapper" style="visibility: visible;">
          <svg width="1000px" height="1000px">
            <path class="path1" d="M 300 400 L 700 400 C 900 400 900 750 600 850 A 400 400 0 0 1 200 200 L 800 800"></path>
            <path class="path2" d="M 300 500 L 700 500"></path>
            <path class="path3" d="M 700 600 L 300 600 C 100 600 100 200 400 150 A 400 380 0 1 1 200 800 L 800 200"></path>
          </svg>
          <button id="menu-icon-trigger" class="menu-icon-trigger"></button>
        </div>
        <div class="navbar-item left-menu-icon-wrapper">
          Menu
        </div>
      </a>

      
    </div></div>
</nav>
<nav id="navbar-clone" class="navbar is-fresh is-transparent" role="navigation" aria-label="main navigation">
  <div class="container">
      <div class="navbar-brand">
  
        
        
        
        <a class="navbar-item">
          <div class="menu-icon-wrapper left-menu-icon-wrapper" style="visibility: visible;">
            <svg width="1000px" height="1000px">
              <path class="path1" d="M 300 400 L 700 400 C 900 400 900 750 600 850 A 400 400 0 0 1 200 200 L 800 800"></path>
              <path class="path2" d="M 300 500 L 700 500"></path>
              <path class="path3" d="M 700 600 L 300 600 C 100 600 100 200 400 150 A 400 380 0 1 1 200 800 L 800 200"></path>
            </svg>
            <button id="menu-icon-trigger" class="menu-icon-trigger"></button>
          </div>
          <div class="navbar-item left-menu-icon-wrapper">
            Menu
          </div>
        </a>
  
        
      </div>
  
      

    </div>
  </nav>
<section class="section is-medium">
  <div class="container">
    <div class="columns">
      <div class="column is-centered-tablet-portrait">
        <h1 class="title is-2 section-title">Constrained Learning for Wireless Systems</h1>
        <h5 class="subtitle is-5 is-muted"></h5>
        <div class="divider"></div>
        
        <section class="section content has-text-justified">
          <h1 id="introduction">Introduction</h1>
<p>This post introduces some work done in collaboration with Mark Eisen about solving optimization problems of the form</p>
<p>$$min_\theta \text{ } E_h[l(h, f_\theta(h))] \<br>
s.t. E_h[g(h, f_\theta(h))] \leq 0$$</p>
<p>where \( l \) is a smooth loss function, and \(g\) is a smooth constraint function. In particular, this optimization has to be done where \(l\) and \(g\) may be unknown or hard to measure. This is the case for wireless power allocation. One goal is to maximize throughput while under power constraints. The throughput is affected by the environment (how the waves bounce around, other wireless interference, etc.) which can be changing over time. The characterstics of the environment can be measured, but not so easily, and it can change quite fast. Thus, Reinforcement Learning (RL) seems to be a good candidate solution. However, most Reinforcement learning algorithms do not optimize with constraints in mind. This work will extend common methods into constrained versions. Our paper is available <a href="#eisen">here</a>.</p>
<h1 id="background">Background</h1>
<p>I am not an expert in wireless technology. So for a more complete picture, I will point you to examples in <a href="https://arxiv.org/abs/1807.08088">our paper</a> that my coauthor has written. A brief summary will be that wireless transceivers have a variety of channels (frequency allocations) on which it can choose to communicate on. The channels are effected by \(h\), the <a href="https://en.wikipedia.org/wiki/Fading">fading</a>. Generally, sending a more powerful signal will allow the message to be recieved with greater probability, though other transmitters might interfere as well. The goal is to allocate power to multiple transmitters across channels to maximize throughput while obeying a power constraint.</p>
<p>Reinforcement learning is a technique to solve <a href="https://en.wikipedia.org/wiki/Markov_decision_process">Markov Decision Processes (MDP)</a> when some part of the MDP is unknown. There are many great resources for an intro into reinforcement learning such as <a href="#sutton">this book</a>. I will abstain from adding one more slightly worse introduction.</p>
<h1 id="method">Method</h1>
<p>We use policy gradient methods from Reinforcement Learning to find a good policy for wireless allocation. The policy is stochastic, and takes a measurement of the fading environment as input. The output is a distribution over power allocations, represented by the parameters of a truncated Gaussian.</p>
<p>Because we have a constrained problem, just applying policy gradient doesn&rsquo;t work. Instead, we look at the <a href="https://en.wikipedia.org/wiki/Duality_(optimization)">dual problem</a>. If you are unfamiliar with optimization and duality, this is a large topic to try to breach here. A high level overview is that optimization problems can be represented in a different (dual) form that may be easier to solve. This dual form takes the constraints of the original and puts it into the objective. The optimization is then done over Lagrange multipliers. One thing to note is that solving the dual problem is not always the same as solving the original problem. In the case of <a href="https://en.wikipedia.org/wiki/Convex_optimization">convex problems</a>, they are the same. This paper shows that with large enough neural networks to represent the policy, the dual problem is <em>almost</em> the same as solving the original.
The dual problem looks like this</p>
<p>$$max_\lambda min_\theta E_h[l(h, f_\theta(h))] + \lambda^T E_h[g(h, f_\theta(h))]\<br>
s.t. \lambda \geq 0 $$</p>
<p>There is a maximization over the Lagrange multipliers, \(\lambda\), and a minimization over the parameters of the policy, \(\theta\). We can treat the Lagrangian (everything within the max-min) as a reward for policy optimizaiton. We can then interleave minimizations, by taking steps with the policy gradient, with maximizations with respect to the lagrange multipliers. This way of alternating optimizations is known as a primal-dual method, and is known method in optimization.</p>
<h1 id="results">Results</h1>
<p>Fig. 1 shows the results of our experiments. We simulated wireless channels with noise, and compared some strategies with the learned policy. The x axis represents time. Our policy gets better with time as it learns from the accumulated experience. The strategies compared with are 1) random allocation, 2) equal power allocation, 3) WMMSE. WWMSE is a state-of-the-art method that requires a model of the capacity function (which our method does not have access to). We can do this in simulation, because we can choose a capacity function and give it to WWMSE. The graphs show that we can obtain comparable performance to WMMSE without having to model the capacity function.</p>
<figure display="table">
  <img src="images/results.png" />
  <figcaption display="table-caption" caption-side="bottom"><i>Fig. 1: Results of applying our method on a wireless channel simulation. </i></figcaption>
</figure>
<h1 id="conclusion">Conclusion</h1>
<p>This work shows that there is some promise in using Reinforcement Learning to optimize wireless power allocation. Since this paper&rsquo;s publication, Mark has <a href="#eisen2020">extended it to use graph neural networks</a>.</p>
<h1 id="references">References</h1>
<p><a name="eisen" href="https://arxiv.org/abs/1807.08088" target="_blank">Eisen, Mark, et al. &ldquo;Learning optimal resource allocations in wireless systems.&rdquo; IEEE Transactions on Signal Processing 67.10 (2019): 2775-2790.</a></p>
<p><a name="sutton" href="http://incompleteideas.net/book/the-book-2nd.html" target="_blank">Sutton, R. S., Barto, A. G. (2018 ). Reinforcement Learning: An Introduction. The MIT Press.</a></p>
<p><a name="eisen2020" href="https://arxiv.org/pdf/1909.01865.pdf" target="_blank">Eisen, Mark, and Alejandro R. Ribeiro. &ldquo;Optimal wireless resource allocation with random edge graph neural networks.&rdquo; IEEE Transactions on Signal Processing (2020).</a></p>

        </section>
      </div>
    </div>
  </div>  
  <div class="container">
    <div class="columns">
      <div class="column has-text-right is-family-monospace is-centered-tablet-portrait">
        Last modified:&nbsp;21 February 2021</div>
    </div>
  </div></section>
<footer class="footer footer-dark">
  <div class="container">
    <div class="columns">


      <div class="column">
        <div class="column">
          <div class="footer-column">
            <ul class="link-list">
              <li>
                Contact Information:
              </li>
            </ul>
          </div>
        </div>
      </div>

      <div class="column">
        <div class="column">
          <div class="footer-column">
            <ul class="link-list">  
              
              <li>
                <a href="mailto:clarkjzhang%20%28at%29%20gmail.com" target="_blank">
                  <span class="icon"><i class="fa fa-envelope"></i></span>
                  
                    clarkjzhang (at) gmail.com
                  
                </a>
              </li>
              
            </ul>
          </div>
        </div>
      </div>

      <div class="column">
        <div class="column">
          <div class="footer-column">
            <ul class="link-list">
              
              <li>
                <a href="https://www.linkedin.com/in/clark-zhang-2207a774" target="_blank">
                  <span class="icon"><i class="fa fa-linkedin"></i></span>
                  
                    LinkedIn
                  
                </a>
              </li>
              
            </ul>
          </div>
        </div>
      </div>

    <div class="column">
      <div class="column">
        <div class="footer-column">
          <ul class="link-list">
            
            <li>
              <a href="https://github.com/chickensouple" target="_blank">
                <span class="icon"><i class="fa fa-github"></i></span>
                
                  Github
                
              </a>
            </li>
            
          </ul>
        </div>
      </div>
    </div>

    </div>
  </div>
</footer>
    <div id="backtotop"><a href="#"></a></div><div class="sidebar scroll">
  <div class="sidebar-header">
    
    <img src="../../images/chicken.png" alt="">
    <a class="sidebar-close" href="javascript:void(0);">
      <i data-feather="x"></i>
    </a>
    
  </div>
  <div class="inner">
    <ul class="sidebar-menu">
      <li class="no-children"><a href="../../"><div class="columns">
          <table width="100%">  
            <tr>
              <td class="">
                <span class="icon"><i class="fa fa-caret-right"></i></span>
                Home
              </td>
              <td class="has-text-right" >
                  
              </td>
            </tr>
          </table>
        </div>
      </a>
      </li>
      <li class="no-children"><a href="../../posts/"><div class="columns">
          <table width="100%">  
            <tr>
              <td class="">
                <span class="icon"><i class="fa fa-caret-right"></i></span>
                Posts
              </td>
              <td class="has-text-right" >
                  
              </td>
            </tr>
          </table>
        </div>
      </a>
      </li>
    </ul>
  </div>
</div>
<script src="../../js/jquery-2.2.4.min.dead7ea11aa9f68488bfe2f9a1be00b7773bea3874f0a221cb898f5c4b041b6a.js" integrity="sha256-3q1&#43;oRqp9oSIv&#43;L5ob4At3c76jh08KIhy4mPXEsEG2o="></script>
  <script src="../../js/feather.4.22.0.min.df46a360906c5f1e61c5867bb42b1f7a8b0166f8351e175a1a4f2958f331e42e.js" integrity="sha256-30ajYJBsXx5hxYZ7tCsfeosBZvg1HhdaGk8pWPMx5C4="></script>
  <script src="../../js/modernizr-3.6.0.min.2f19d51e4b89c87bdabcdb05684b34f3c1ac6e5e3919ae1c067377c4f64f23cc.js" integrity="sha256-LxnVHkuJyHvavNsFaEs088Gsbl45Ga4cBnN3xPZPI8w="></script>
  <script src="../../js/refresh.min.58d0ebd018c8eacfa18b730fb90e94bc3e3d1b009ae12f99fb0f8f5f4bf6a581.js" integrity="sha256-WNDr0BjI6s&#43;hi3MPuQ6UvD49GwCa4S&#43;Z&#43;w&#43;PX0v2pYE="></script><script>
  var mathjax_pagemacros = (typeof mathjax_pagemacros == 'undefined') ? [] : mathjax_pagemacros;

  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      processEscapes: true,
    },
    svg: {
      fontCache: 'global'
    },
    loader: {load: ['[tex]/color', '[tex]/configMacros']},
    tex: {
      packages: {'[+]': ['color', 'configMacros']},
      macros: mathjax_pagemacros
    },
  };
  
</script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script></body>
</html>
