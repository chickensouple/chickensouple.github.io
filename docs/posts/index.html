<!DOCTYPE html>
<html lang="en-us">
  <head><link rel="icon" href="../chickensouple.github.io/chickensouple.github.io/favicon_main.svg"><meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
<meta http-equiv="x-ua-compatible" content="ie=edge" /><title>Clark Zhang - Posts</title>

    
<link href="https://fonts.googleapis.com/css?family=Open&#43;Sans" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=Open&#43;Sans" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=Open&#43;Sans" rel="stylesheet"><link rel="stylesheet" type="text/css" href="../chickensouple.github.io/chickensouple.github.io/css/style.143de6bfc6cfe35f12dec288425de51e8f1e02c5adff32a6d1cf48709141500b.css" integrity="sha256-FD3mv8bP418S3sKIQl3lHo8eAsWt/zKm0c9IcJFBUAs=">
<link rel="stylesheet" type="text/css" href="../chickensouple.github.io/chickensouple.github.io/css/monokai-sublime.9.15.8.min.91376415864fdd3a92be524052267afece4bdb1bb8c6c754f5e60c5ac28e93be.css" integrity="sha256-kTdkFYZP3TqSvlJAUiZ6/s5L2xu4xsdU9eYMWsKOk74=">
<link rel="stylesheet" type="text/css" href="../chickensouple.github.io/chickensouple.github.io/css/icons.f72c0103ff9598115ab2a76351494b950570479ce3f1795fec13fc62e4185dcf.css" integrity="sha256-9ywBA/&#43;VmBFasqdjUUlLlQVwR5zj8Xlf7BP8YuQYXc8=">
<link rel="stylesheet" type="text/css" href="../chickensouple.github.io/chickensouple.github.io/css/refresh.ccfd23f8053a1571a3e474a6877f42a6a1924c6bbf1b64c7704b49a0353b4650.css" integrity="sha256-zP0j&#43;AU6FXGj5HSmh39CpqGSTGu/G2THcEtJoDU7RlA=">
<link rel="stylesheet" type="text/css" href="../chickensouple.github.io/chickensouple.github.io/css/devicon.min.149016fbf45c8bc157d6f55ce3ee875feaa3f90446bf7d151fbc16a9f21a8859.css" integrity="sha256-FJAW&#43;/Rci8FX1vVc4&#43;6HX&#43;qj&#43;QRGv30VH7wWqfIaiFk=">
    

  </head>
  <body>
     

    <div id="preloader">
      <div id="status"></div>
    </div><nav class="navbar is-fresh is-transparent no-shadow" role="navigation" aria-label="main navigation">
  <div class="container">
    <div class="navbar-brand">

      
      
      
      <a class="navbar-item">
        <div class="menu-icon-wrapper left-menu-icon-wrapper" style="visibility: visible;">
          <svg width="1000px" height="1000px">
            <path class="path1" d="M 300 400 L 700 400 C 900 400 900 750 600 850 A 400 400 0 0 1 200 200 L 800 800"></path>
            <path class="path2" d="M 300 500 L 700 500"></path>
            <path class="path3" d="M 700 600 L 300 600 C 100 600 100 200 400 150 A 400 380 0 1 1 200 800 L 800 200"></path>
          </svg>
          <button id="menu-icon-trigger" class="menu-icon-trigger"></button>
        </div>
        <div class="navbar-item left-menu-icon-wrapper">
          Menu
        </div>
      </a>

      
    </div></div>
</nav>
<nav id="navbar-clone" class="navbar is-fresh is-transparent" role="navigation" aria-label="main navigation">
  <div class="container">
      <div class="navbar-brand">
  
        
        
        
        <a class="navbar-item">
          <div class="menu-icon-wrapper left-menu-icon-wrapper" style="visibility: visible;">
            <svg width="1000px" height="1000px">
              <path class="path1" d="M 300 400 L 700 400 C 900 400 900 750 600 850 A 400 400 0 0 1 200 200 L 800 800"></path>
              <path class="path2" d="M 300 500 L 700 500"></path>
              <path class="path3" d="M 700 600 L 300 600 C 100 600 100 200 400 150 A 400 380 0 1 1 200 800 L 800 200"></path>
            </svg>
            <button id="menu-icon-trigger" class="menu-icon-trigger"></button>
          </div>
          <div class="navbar-item left-menu-icon-wrapper">
            Tags
          </div>
        </a>
  
        <div class="navbar-item is-expanded"></div>
        <a class="navbar-item is-hidden-desktop">  
          <div data-target="cloned-navbar-menu" class="navbar-item right-menu-icon-wrapper is-hidden-desktop">
            Menu
          </div>
          <div data-target="cloned-navbar-menu" class="menu-icon-wrapper right-menu-icon-wrapper" style="visibility: visible;">
            <svg width="1000px" height="1000px">
              <path class="path1" d="M 300 400 L 700 400 C 900 400 900 750 600 850 A 400 400 0 0 1 200 200 L 800 800"></path>
              <path class="path2" d="M 300 500 L 700 500"></path>
              <path class="path3" d="M 700 600 L 300 600 C 100 600 100 200 400 150 A 400 380 0 1 1 200 800 L 800 200"></path>
            </svg>
            <button id="menu-icon-trigger" class="menu-icon-trigger"></button>
          </div>
        </a>
      </div><div id="cloned-navbar-menu" class="navbar-menu is-static">
        <div class="navbar-end"><a href="../chickensouple.github.io/posts/" class="navbar-item is-secondary">Posts</a>
          
          
        
        </div>
      </div>
    </div>
  </nav>
<section class="section is-medium">
        <div class="container">
          
            
              <h1 class="title section-title">Posts</h1>
              <h5 class="subtitle is-5 is-muted"></h5>
              <div class="divider"></div>        
              <div class="section is-small">
                <div class="columns">
                    <div class="column is-4"><div class="box">
      <figure class="image is-3by2">
        <a href="../chickensouple.github.io/posts/constrained_wireless/">
        
           
           
          
            <img src="../chickensouple.github.io/chickensouple.github.io/posts/constrained_wireless/images/cap_lambd_hu2887ca7646cd42e51b7ffd8624bbbc77_28779_600x0_resize_box_2.png" style="width: auto; margin:auto" alt="">
          
        
        </a>
      </figure>
      <h3 class="title is-5 refresh-summary-title">
        Constrained Learning for Wireless Systems
      </h3>
      <p class="refresh-summary"> Introduction This post introduces some work done in collaboration with Mark Eisen about solving optimization problems of the form
$$min_\theta \text{ } E_h[l(h, f_\theta(h))] \
s.t. E_h[g(h, f_\theta(h))] \leq 0$$
where $$l$$ is a smooth loss function, and $$g$$ is a smooth constraint function. In particular, this optimization has to be done where $$l$$ and $$g$$ may be unknown or hard to measure. This is the case for wireless power allocation. One goal is to maximize throughput while under power constraints. The throughput is affected by the environment (how the waves bounce around, other wireless interference, etc.) which can be changing over time. The characterstics of the environment can be measured, but not so easily, and it can change quite fast. Thus, Reinforcement Learning (RL) seems to be a good candidate solution. However, most Reinforcement learning algorithms do not optimize with constraints in mind. This work will extend common methods into constrained versions. Our paper is available here.
Background I am not an expert in wireless technology. So for a more complete picture, I will point you to examples in our paper that my coauthor has written. A brief summary will be that wireless transceivers have a variety of channels (frequency allocations) on which it can choose to communicate on. The channels are effected by $$h$$, the fading. Generally, sending a more powerful signal will allow the message to be recieved with greater probability, though other transmitters might interfere as well. The goal is to allocate power to multiple transmitters across channels to maximize throughput while obeying a power constraint.
Reinforcement learning is a technique to solve Markov Decision Processes (MDP) when some part of the MDP is unknown. There are many great resources for an intro into reinforcement learning such as this book. I will abstain from adding one more slightly worse introduction.
Method We use policy gradient methods from Reinforcement Learning to find a good policy for wireless allocation. The policy is stochastic, and takes a measurement of the fading environment as input. The output is a distribution over power allocations, represented by the parameters of a truncated Gaussian.
Because we have a constrained problem, just applying policy gradient doesn&amp;rsquo;t work. Instead, we look at the dual problem. If you are unfamiliar with optimization and duality, this is a large topic to try to breach here. A high level overview is that optimization problems can be represented in a different (dual) form that may be easier to solve. This dual form takes the constraints of the original and puts it into the objective. The optimization is then done over Lagrange multipliers. One thing to note is that solving the dual problem is not always the same as solving the original problem. In the case of convex problems, they are the same. This paper shows that with large enough neural networks to represent the policy, the dual problem is almost the same as solving the original. The dual problem looks like this
$$max_\lambda min_\theta E_h[l(h, f_\theta(h))] &#43; \lambda^T E_h[g(h, f_\theta(h))]\
s.t. \lambda \geq 0 $$
There is a maximization over the Lagrange multipliers, $$\lambda$$, and a minimization over the parameters of the policy, $$\theta$$. We can treat the Lagrangian (everything within the max-min) as a reward for policy optimizaiton. We can then interleave minimizations, by taking steps with the policy gradient, with maximizations with respect to the lagrange multipliers. This way of alternating optimizations is known as a primal-dual method, and is known method in optimization.
Results Fig. 1 shows the results of our experiments. We simulated wireless channels with noise, and compared some strategies with the learned policy. The x axis represents time. Our policy gets better with time as it learns from the accumulated experience. The strategies compared with are 1) random allocation, 2) equal power allocation, 3) WMMSE. WWMSE is a state-of-the-art method that requires a model of the capacity function (which our method does not have access to). We can do this in simulation, because we can choose a capacity function and give it to WWMSE. The graphs show that we can obtain comparable performance to WMMSE without having to model the capacity function.
Fig. 1: Results of applying our method on a wireless channel simulation.   Conclusion This work shows that there is some promise in using Reinforcement Learning to optimize wireless power allocation. Since this paper&amp;rsquo;s publication, Mark has extended it to use graph neural networks.
References Eisen, Mark, et al. &amp;ldquo;Learning optimal resource allocations in wireless systems.&amp;rdquo; IEEE Transactions on Signal Processing 67.10 (2019): 2775-2790.
Sutton, R. S., Barto, A. G. (2018 ). Reinforcement Learning: An Introduction. The MIT Press.
Eisen, Mark, and Alejandro R. Ribeiro. &amp;ldquo;Optimal wireless resource allocation with random edge graph neural networks.&amp;rdquo; IEEE Transactions on Signal Processing (2020).
</p> 
      <div class="action has-text-right">
        <a href="../chickensouple.github.io/posts/constrained_wireless/" class="button is-primary">
                Read more
            </a>
      </div>
    </div>
  </div>
<div class="column is-4"><div class="box">
      <figure class="image is-3by2">
        <a href="../chickensouple.github.io/posts/learning_implicit/">
        
           
           
          
            <img src="../chickensouple.github.io/chickensouple.github.io/posts/learning_implicit/images/main_hu05575220a12ae26b9d90594b61ed693c_62187_600x0_resize_box_2.png" style="height: auto; margin:auto" alt="">
          
        
        </a>
      </figure>
      <h3 class="title is-5 refresh-summary-title">
        Learning Implicit Sampling Distributions
      </h3>
      <p class="refresh-summary">Introduction Robot motion planning is a problem that has been studied for many decades. For quite a while, sampling based planning approaches such as Rapidly Exploring Random Trees (RRT), Expansive Space Trees (EST), were quite popular in academia have been used with great success in some problem domains. This post introduces work I have done in collaboration with Jinwook Huh and Daniel D. Lee about speeding up such motion planning approaches by learning a better sampling distribution to draw from. This work is available here. While many more modern planners for robots such as autonomous vehicles no longer have a basis in these sampling based planners, perhaps similar ideas in how to integrate machine learning into an existing planning framework can be useful.
Background We can discuss a bare-bones planning problem. You have a map of the environment that can be represented maybe as a point cloud or occupancy map or collection of polygons, or a signed distance function, etc. You have a goal state and a starting state, and you would like to find a set of actions for a robot that will take you from start to goal.
Fig. 1: An example of a planning problem. The green dot is the start, while the red is the goal.  For sampling-based planners, the general idea is to explore a continuous state space with random samples. The prototypical sampling based planner, RRT, roughly works as follows.
 Initialize a tree with a node that represents the starting state Sample a point in the entire state space Find the closest node in your tree to the sample From the closest point, move towards the random sample and add the new state as a node in the tree Repeat steps 2-4 until the goal is included in the tree.  This process will grow the tree to cover the space and hopefully find a path from the start to the goal. You can check out Steven LaValle&amp;rsquo;s website (The guy who invented RRT&amp;rsquo;s) for cool pictures of RRT in action and what it looks like while the algorithm is running.
For these sampling based planners, Step 2 in the algorithm can be quite important. It is what will guide how the tree searches the environment (if you are familiar with A* and its many variants, the sampling distribution plays a similar role to the heuristic function). Typically, a uniform distribution with a slight goal bias (with some chance, choose the goal as the random sample) is used. There have been many efforts to come up with better sampling techniques including using the medial axis, visibility, or volumes. These are all heuristics that hopefully are better distributions to sample from than the uniform distribution.
A natural question arises, can we learn more general heuristics for common classes of planning problems? For example, if we are using a robot arm to manipulate objects on a table, we might be able to exploit some of the structure of the problem to speed up our search. This heuristic may be different if you are talking about a robot navigating an office space, or an autonomous vehicle navigating around a city. And it would be great to use data from planning in those different environments to learn good sample distribution.
Method To learn a sampling distribution, we must first be able to represent a sampling distribution. In this case, we have chosen to represent it implicitly (thus the title of the paper) with rejection sampling. That is, given some simple distribution that you can easily draw samples from (uniform with goal bias), assign a probability of rejecting that sample. Then when you are sampling from this implicitly defined distribution, you will draw samples from the original distribution and with some probability, reject on those samples. This will define a new distribution that can look quite different from the original distribution with out rejecting any samples.
In this particular case, we use a small (tiny by modern standards) neural network to process some features of a sample drawn. The output of the neural network is a probability to accept or reject.
Fig. 2: A tiny neural network.  We can then view the process of accepting and rejecting samples to pass to the planner as a Markov Decision Process (MDP) where the state is the state of the planner with a new random sample, and the actions are to reject or accept it. The rewards are chosen to reward finding a path faster. This process can be optimized with some standard Reinforcement Learning (RL) techniques. There are a great many tutorials and introductions to RL, so I will abstain from adding one more slightly worse introduction. I highly recommend this book by Sutton as a good introductory read. For our purpoes, we will use a simple policy gradient method to optimize our rejection &amp;ldquo;policy.&amp;rdquo;
Results This methodology is applied to several environments and algorithms and the results are shown in Fig. 3. In addition to RRT, you can apply this rejection sampling technique to any other sampling-based planner such as Bi-RRT or EST, etc. The flytrap environment refers to Fig. 1, while Pendulum referes to an inverted pendulum.
Fig. 3: Results in simulation.  We also applied this to a robot arm in real life.
Fig. 4: Thor&#39;s robot arm.  Fig. 5: Results on a robot arm.  In these cases, the new sampling distribution can reduce the number of collision checks by rejecting samples that are highly likely to induce them as well as reducing overall execution time.
It is also interesting to look at the learned rejection policies more closely. For the flytrap environment, good rejection policies actually depend on the size of the flytrap relative to the environment. The visibility based heuristic works when the flytrap is small, and the volume based heuristic works when the flytrap is big. When running the rejection policy optimization over small and big flytrap environments, it actually obtains very similar heuristics to the human created ones.
Fig. 6: Learned rejection policies vs. human heuristics. Flytrap train refers to the small flytrap, dynamic-domain refers to the visibility heuristic, flytrap-balltree refers to the big flytrap, and BallTree refers to the volume based heuristic.  Fig. 7: Cool sampling distributions in action on the flytrap environment.  For the robotic arm, when we look at the learned distribution, the samples that are further away from the table are rejected more often. For the task of moving objects around, it is wasteful to explore random configurations that bring the arm way over or away from the objects.
Conclusion This work had the goal to unify the understanding of how a lot of previous sampling heuristics worked. Many use rejection sampling, and Reinforcement Learning can be applied to optimize that rejection sampling so as to automate the process of finding heuristics. There are some interesting tangents to discuss. For example, if an optimal planner is available (RRT* and friends), instead of using RL to optimize rejection policies, supervised learning can be used instead. With supervision, the policies can be learned faster, and perhaps, we can obtain even better policies. For complicated environments, however, RRT* can take very long to run. It can be interesting to start with RL and move to supervised learning as our heuristics get better and we can find near-optimal policies. Since this paper&amp;rsquo;s publication, there has been interesting work in learning heurstics for A*, as well as generative sampling distributions.
References Zhang, Clark, Jinwook Huh, and Daniel D. Lee. &amp;ldquo;Learning implicit sampling distributions for motion planning.&amp;rdquo; 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2018.
Y. Yang and O. Brock, “Adapting the sampling distribution in prm planners based on an approximated medial axis,” in Robotics and Automation, 2004. Proceedings. ICRA’04. 2004 IEEE International Conference on, vol. 5, pp. 4405–4410, IEEE, 2004.
A. Yershova, L. Jaillet, T. Siméon, and S. M. LaValle, “Dynamic- domain rrts: Efficient exploration by controlling the sampling domain,” in Robotics and Automation, 2005. ICRA 2005. Proceedings of the 2005 IEEE International Conference on, pp. 3856–3861, IEEE, 2005.
A. Shkolnik and R. Tedrake, “Sample-based planning with volumes in configuration space,” arXiv preprint arXiv:1109.3145, 2011.
Sutton, R. S., Barto, A. G. (2018 ). Reinforcement Learning: An Introduction. The MIT Press.
Zlatan Ajanovic, Halil Beglerovic, Bakir Lacevic. &amp;ldquo;A novel approach to model exploration for value function learning.&amp;rdquo; RSS 2019 workshop. 
Ichter, Brian, James Harrison, and Marco Pavone. &amp;ldquo;Learning sampling distributions for robot motion planning.&amp;rdquo; 2018 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2018.
</p> 
      <div class="action has-text-right">
        <a href="../chickensouple.github.io/posts/learning_implicit/" class="button is-primary">
                Read more
            </a>
      </div>
    </div>
  </div>
<div class="column is-4"><div class="box">
      <figure class="image is-3by2">
        <a href="../chickensouple.github.io/posts/sufficiently_accurate/">
        
           
           
          
            <img src="../chickensouple.github.io/chickensouple.github.io/posts/sufficiently_accurate/images/main_huf56d47a1ef50456175de88d0aa5fd9f3_72932_600x0_resize_box_2.png" style="width: auto; margin:auto" alt="">
          
        
        </a>
      </figure>
      <h3 class="title is-5 refresh-summary-title">
        Sufficiently Accurate Model Learning
      </h3>
      <p class="refresh-summary"> Introduction This work looks at learning dynamics model for planning and control. Planners require a model of the system to work with. This model will inform the planner what happens when you take an action at some specific state. In previous work , we had applied machine learning to learn heuristics for planning. This work addresses learning the model for planning. A paper containing some results is available here.
Background Planners require a model of the world that will tell us how it will evolve with certain actions. This can be expressed as a differential equation
$$\dot{x} = f(x, u)$$
or as a discrete time function
$$x_{k&#43;1} = f(x_k, u_k)$$
This work will be using discrete time functions, but the ideas are more generally applicable. Many modern planners will formulate the problem as a nonlinear optimization problem, and solve it with general purpose or more specialized solvers. Examples include CHOMP, TrajOpt, or iLQG. An advantage these planners have over sampling based (such as RRT) or search based (such as A*) planners are that they perform quite admirably in high dimensions and converge quickly to locally optimal solutions. These optimization planners only work in &amp;ldquo;relatively&amp;rdquo; simple environments. Problems where finding the solution might require a search through many homotopy classes are difficult for this class of optimization based planners. They require a decent initialization. But, as it turns out, for many real world situations, planning is more akin to optimizing a general idea of what to do, rather than solving a complicated maze.
One can try to use data to obtain a better model for planning with these optimization based planners. Usually the constraints on the model are that they have to be differentiable. Using a neural network to represent these models fits this constraint quite well. By using data to tune or learn your model, you can have a more accurate picture of what your system looks like and adapt to any physical changes it undergoes. Maybe the wheel of a car becomes more smooth over time, or a motor in a robot arm becomes less powerful. By using data, the planners can adapt to these changes in a more active way.
There is a vast literature on model learning. In controls, there is a huge trove of &amp;ldquo;System Identification&amp;rdquo; papers which mostly focus on learning linear systems. In robotics, there are many methods that fit Gaussian Process Regression models, or Gaussian Mixture Models, or Neural Networks, or linear models to various systems. This paper presents not a new method, but a new objective for which these methods can use. Most of these methods will simply seek to minimize an error between an observed next state, $$x_{k&#43;1}$$ with that the predicted next state, $$\hat{x}_{k&#43;1}$$. This results in a loss functions that look like
$$ \mathbb{E} || x_{k&#43;1} - f_\theta(x_k, u_k) ||_2^2$$
where $$f_\theta$$ is the learned model.
Method Our work proposes that we can solve a constrained problem instead of simply minimizing an unconstrained objective. The constraints can contain prior information about the system, and solving the constrained problem can then lead to models better suited for planners. An simple example is suppose we have a block we are pushing on a table, that has lot of uneven surfaces. Suppose we want to push the block from various points to a common resting location. We can learn a model that is uniformly accurate everywhere. Or, we can learn a model that is selectively good near the goal location. Because far away from the goal, the plan is simple, we push in the direction of the goal. It is only when we are near the goal we care about how much we push so that the block comes to a stop efficiently and correctly. This prior knowledge can be summarized as a constraint on the accuracy near the goal. We can write a constrained problem that looks like
$$ min_\theta \mathbb{E} f(x_k, u_k, x_{k&#43;1})\
s.t. \mathbb{E} g(x_k, u_k, x_{k&#43;1}) \leq 0$$
We have explored solving constrained problems before . In this case, we expand the theory somewhat. We can no longer analytically compute the expectations in the constrained problem and can only use samples of data. This is because the distribution of trajectories for a system is very complicated. This paper shows that you can still obtain small duality gaps with large enough models and large enough numbers of samples.
Similar to our previous work , we can use a primal-dual method to solve this. We alternatively minimize the Lagrangian, and maximize the dual variables.
Results Using some constrained objectives, we can obtain models so that the planner can do better on average. We have tested this in simulation on a state-dependent-friction block pushing task, a ball bouncing task, and a quadrotor landing task. Numerical results show that we can trade off accuracy in unimportant parts of the statespace for better accuracy in others, and obtain better performing planners. The detailed results are under submission right now. But to highlight a few instances for why this is better, we can look at the quadrotor landing. Our constraint is that the model should be more accurate close to the ground. This is where ground effects are most prominant and require greater precision in the model.
Fig. 1: Quadrotor trajectories.  A model learned using our objective is better able to compensate for the greater updraft near the ground when landing and can get to the location more accurately.
References Zhang, Clark, and Paternain, Santiago, &amp;amp; Ribeiro, Alejandro. Sufficiently Accurate Model Learning for Planning. 2021.
Zucker, Matt, et al. &amp;ldquo;Chomp: Covariant hamiltonian optimization for motion planning.&amp;rdquo; The International Journal of Robotics Research 32.9-10 (2013): 1164-1193. 
Schulman, John, et al. &amp;ldquo;Finding Locally Optimal, Collision-Free Trajectories with Sequential Convex Optimization.&amp;rdquo; Robotics: science and systems. Vol. 9. No. 1. 2013.&amp;lt; /a&amp;gt;
Todorov, Emanuel, and Weiwei Li. &amp;ldquo;A generalized iterative LQG method for locally-optimal feedback control of constrained nonlinear stochastic systems.&amp;rdquo; Proceedings of the 2005, American Control Conference, 2005.. IEEE, 2005. 
</p> 
      <div class="action has-text-right">
        <a href="../chickensouple.github.io/posts/sufficiently_accurate/" class="button is-primary">
                Read more
            </a>
      </div>
    </div>
  </div>

                  </div>
                
              </div>
            
          
        </div>
    </section><footer class="footer footer-dark">
  <div class="container">
    <div class="columns">
      <div class="column">
        <img src="../chickensouple.github.io/chickensouple.github.io/footer.svg" alt="">
        
      </div>
      
    <div class="column"> 
        
    
      
      <div class="column">
        <div class="footer-column">
          <div class="footer-header">
              <h3>Contacts</h3>
          </div>
          <ul class="link-list">
            
            <li>
              <a href="https://www.linkedin.com/in/clark-zhang-2207a774" target="_blank">
                <span class="icon"><i class="fa fa-linkedin"></i></span>
                
                  LinkedIn
                
              </a>
            </li>
            
            
            <li>
              <a href="https://github.com/chickensouple" target="_blank">
                <span class="icon"><i class="fa fa-github"></i></span>
                
                  Github
                
              </a>
            </li>
                         
            
            <li>
              <a href="mailto:N/A" target="_blank">
                <span class="icon"><i class="fa fa-envelope"></i></span>
                
                  clarkjzhang (at) gmail.com
                
              </a>
            </li>
            
                   
                   
                   
          </ul>
        </div>
      </div>
      

      

    </div>
  </div>
</footer>
    <div id="backtotop"><a href="#"></a></div><div class="sidebar scroll">
  <div class="sidebar-header"><img src="../chickensouple.github.io/chickensouple.github.io/sidebar.svg" alt="">
    
    <a class="sidebar-close" href="javascript:void(0);">
      <i data-feather="x"></i>
    </a>
  </div>
  <div class="inner">
    <ul class="sidebar-menu">
      <li class="no-children"><a href="chickensouple.github.io/"><div class="columns">
          <table width="100%">  
            <tr>
              <td class="">
                <span class="icon"><i class="fa fa-cubes"></i></span>
                Home
              </td>
              <td class="has-text-right" >
                  
              </td>
            </tr>
          </table>
        </div>
      </a>
      </li>
      <li class="no-children"><a href="chickensouple.github.io/posts/"><div class="columns">
          <table width="100%">  
            <tr>
              <td class="">
                <span class="icon"><i class="fa fa-cubes"></i></span>
                Posts
              </td>
              <td class="has-text-right" >
                  
              </td>
            </tr>
          </table>
        </div>
      </a>
      </li>
    </ul>
  </div>
</div>
<script src="../chickensouple.github.io/chickensouple.github.io/js/jquery-2.2.4.893e90f6230962e42231635df650f20544ad22affc3ee396df768eaa6bc5a6a2.js" integrity="sha256-iT6Q9iMJYuQiMWNd9lDyBUStIq/8PuOW33aOqmvFpqI="></script>
  <script src="../chickensouple.github.io/chickensouple.github.io/js/feather.4.22.0.1ab07abeb9975f283f6b5f29451981be680fbf77ea778f991d457511d210476a.js" integrity="sha256-GrB6vrmXXyg/a18pRRmBvmgPv3fqd4&#43;ZHUV1EdIQR2o="></script>
  <script src="../chickensouple.github.io/chickensouple.github.io/js/modernizr-3.6.0.e013a1e54e3c19d83537ba42b900d34451e5e4b2e789be27a02ac9b152edb741.js" integrity="sha256-4BOh5U48Gdg1N7pCuQDTRFHl5LLnib4noCrJsVLtt0E="></script>
  <script src="../chickensouple.github.io/chickensouple.github.io/js/refresh.62c1a9b7d85bcf4d944cd585a01dfa8fb15112a7d2cf9fb819db20493bfe7484.js" integrity="sha256-YsGpt9hbz02UTNWFoB36j7FREqfSz5&#43;4GdsgSTv&#43;dIQ="></script><script>
  window.MathJax = {
    loader: {
      load: ['core', 'input/tex-base', 'output/chtml'],  
      source: {
        'core': '\/chickensouple.github.io\/chickensouple.github.io\/js\/mathjax\/core.d48fedf25c74c54fa6bf79646de92b02155872bdc4f5f7d0bbfc662523d8b4f5.js',
        'input/tex-base': '\/chickensouple.github.io\/chickensouple.github.io\/js\/mathjax\/tex-base.1b68b8741dfc54e8f7222f88bea8ffcfc57ac54b2a2d8f4edf6800aa44d49441.js',
        'output/chtml': '\/chickensouple.github.io\/chickensouple.github.io\/js\/mathjax\/chtml.926cd166e0f8c1f8a566a718d60c9c58a2b7142b156d30d5f02cec7c3b0ad60a.js',
        'output/chtml/fonts/tex': '\/chickensouple.github.io\/chickensouple.github.io\/js\/mathjax\/tex_out.b8b2bb939c0dae84bf1390bfe6d32af13e83f647cdac01d544d2a7a517477e9d.js'
      },
    },
    chtml: {
      fontURL: '/fonts' 
    },
  };
</script><script src="../chickensouple.github.io/chickensouple.github.io/js/mathjax/startup.234a2513e6bdbc1eee06ca19abceca30fe4034e82afb373c08274c1ba2feb1a6.js" integrity="sha256-I0olE&#43;a9vB7uBsoZq87KMP5ANOgq&#43;zc8CCdMG6L&#43;saY="></script>
  <script src="../chickensouple.github.io/chickensouple.github.io/js/highlight.9.18.1.b1c58829c55afcc1f568022af4b08ed8976da404d2990b7535bd8e19c0e3310c.js" integrity="sha256-scWIKcVa/MH1aAIq9LCO2JdtpATSmQt1Nb2OGcDjMQw="></script>
  <script src="../chickensouple.github.io/chickensouple.github.io/js/highlightjs-line-numbers.2.7.0.min.ddfe282e07b7ec1ed069c23f92c7c8216ddb3f1879c4e962d37fd52adbd15a05.js" integrity="sha256-3f4oLge37B7QacI/ksfIIW3bPxh5xOli03/VKtvRWgU="></script><script>
  hljs.initHighlightingOnLoad();
  hljs.initLineNumbersOnLoad();
  document.addEventListener('DOMContentLoaded', (event) => {
    document.querySelectorAll('.codeinline').forEach((block) => {
      hljs.highlightBlock(block);
    });
  });
</script>

</body>
</html>
